{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepLearning import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import treys\n",
    "from treys import Evaluator\n",
    "from treys import Card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM poker bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so what are we trying to accomplish here.\n",
    "We want to build a naive LSTM poker bot. We need to do three things:\n",
    "\n",
    "- [x] Encode a poker game in a meaningful way\n",
    "\n",
    "- [x] Develop a useful loss metric for games\n",
    "\n",
    "- [x] Test many-to-many or many-to-one models for games\n",
    "\n",
    "Note: the above checkboxes illustrate a CURSORY look at these topics. We will continue to explore them in future games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding a poker game:\n",
    "\n",
    "We're going to approach this by just encoding the cards at each step.\n",
    "There are 4 discrete betting stages that are worth keeping track of in poker:\n",
    "- Pre-Flop\n",
    "- Flop\n",
    "- Turn\n",
    "- River\n",
    "\n",
    "We'll create a `1x4x52` dim tensor to encode all the information in each stage for in one poker game.\n",
    "\n",
    "This way we can create a training dataframe of `N` examples with shape: `Nx4x52`\n",
    "Where `4` again represents the discrete betting stages and `52` is the one hot encoded cards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_staged_games(num_games, device = 'cpu', dtype = torch.float, verbose = False):\n",
    "    to = {'device': device, 'dtype':dtype}\n",
    "    X = []\n",
    "    y = []\n",
    "    for g in range(num_games//2):\n",
    "        start_time = time.time()\n",
    "        if g % verbose == 0:\n",
    "            print(\"Completed {} in {:2f} seconds\".format(verbose, time.time()-start_time))\n",
    "        p1, p2, board = utils.make_heads_up()\n",
    "        g1 = torch.stack(\n",
    "            [\n",
    "                utils.one_hot_cards(p1, **to), \n",
    "                utils.one_hot_cards(board[:3], **to), \n",
    "                utils.one_hot_cards([board[3]], **to),\n",
    "                utils.one_hot_cards([board[4]], **to)\n",
    "            ]\n",
    "        )\n",
    "        g2 = torch.stack(\n",
    "            [\n",
    "                utils.one_hot_cards(p2, **to), \n",
    "                utils.one_hot_cards(board[:3], **to), \n",
    "                utils.one_hot_cards([board[3]], **to),\n",
    "                utils.one_hot_cards([board[4]], **to)\n",
    "            ]\n",
    "        )\n",
    "        X.append(g1)\n",
    "        X.append(g2)\n",
    "        \n",
    "        s1, s2 = utils.score_heads_up(p1, p2, board)\n",
    "        \n",
    "        y.append(torch.tensor(s2, **to)) # if p1 wins append 0 => s2 is 0\n",
    "        y.append(torch.tensor(s1, **to)) # if p1 loses append 1 => s1 is 1\n",
    "        \n",
    "    X = torch.stack(X)\n",
    "    y = torch.stack(y)\n",
    "    \n",
    "    return X, y.to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to One architecture experiments:\n",
    "\n",
    "We're going to try to use a Many-to-one LSTM to predict our likelihood of winning a hand.\n",
    "\n",
    "Our approach is as follows:\n",
    "- LSTM with a hidden dimension of 300 (arbitrary)\n",
    "- Output transformation with the following properties:\n",
    "    - Linear layer with input 300 and output 2\n",
    "    - Softmax for thresholding \n",
    "    - First dim of output is likelihood for us to win\n",
    "    - Second dim is likelihood for opponents to win\n",
    "    - This allows us to use `CrossEntropyLoss` as a penalty mechanism (Note we'll have to skip the softmax when we use `CrossEntropyLoss`)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A simpleLSTM for poker\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size = 300):\n",
    "        \"\"\"\n",
    "        hidden_size: size of hidden dimension of LSTM\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTMCell(52, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 2)\n",
    "        self.squash = nn.Sigmoid()\n",
    "#         self.params = []\n",
    "#         self.params += list(self.lstm.parameters()) + list(self.output.parameters()) + list(self.squash.parameters())\n",
    "        \n",
    "    def forward(self, X, squash = False):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM\n",
    "        X: input of shape Nx4x52\n",
    "        squash: whether or not to squash using softmax\n",
    "        \n",
    "        Returns:\n",
    "            tensor of shape Nx2 representing likelihood to win and likelihood for opponent to win.\n",
    "            IF squash==True values represent probabilities.\n",
    "        \"\"\"\n",
    "        N, r, _ = X.shape\n",
    "        \n",
    "        hand = X[:,0]\n",
    "        flop = X[:,1]\n",
    "        turn = X[:,2]\n",
    "        river = X[:,3]\n",
    "        \n",
    "        # now we can pass through:\n",
    "        hidden, cell = torch.zeros(N, 300, dtype=X.dtype, device=X.device), torch.zeros(N, 300, dtype=X.dtype, device=X.device)\n",
    "        \n",
    "        # first round:\n",
    "        hidden, cell = self.lstm(hand, (hidden,cell))\n",
    "        \n",
    "        # second round:\n",
    "        hidden, cell = self.lstm(flop, (hidden, cell))\n",
    "        \n",
    "        # third round:\n",
    "        hidden, cell = self.lstm(turn, (hidden, cell))\n",
    "        \n",
    "        # fourth round:\n",
    "        hidden, cell = self.lstm(river, (hidden, cell))\n",
    "        \n",
    "        #output:\n",
    "        scores = self.output(hidden)\n",
    "        \n",
    "        if squash:\n",
    "            return self.squash(scores)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model, epochs = 10, verbose = False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: training tensor of shape (N,52)\n",
    "        y: target tensor of shape (N,1)\n",
    "        model: a torch.nn.Module model\n",
    "        epochs: number of epochs to train\n",
    "        verbose: iterations of epochs to print out (1 for all, False for none)\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        scores = model(X)\n",
    "        loss = criterion(scores, y)\n",
    "        with torch.no_grad():\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if verbose and e%verbose == 0:\n",
    "                print(\"epoch: {} loss: {:.4f}\".format(e,loss.item()))\n",
    "                \n",
    "def evaluate_model(X, y, model):\n",
    "    with torch.no_grad():\n",
    "        scores = model(X)\n",
    "        #threshold for predictions\n",
    "        scores[scores < 0.5] = 0\n",
    "        scores[scores >= 0.5] = 1\n",
    "        return (scores == y).sum().item()/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING THIS TAKES A LONG TIME\n",
    "X_train, y_train = make_staged_games(30000, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = make_staged_games(1000, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleLSTM().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.5587\n",
      "epoch: 100 loss: 0.6666\n",
      "epoch: 200 loss: 0.6477\n",
      "epoch: 300 loss: 0.6878\n",
      "epoch: 400 loss: 0.6623\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train, y_train, model, 500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.629"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out == y_test).sum().item()/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deepLearning/lstmTrain.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deepLearning/lstmTrain_y.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Loss experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have our model make a many-to-many predictions for each round of the betting.\n",
    "\n",
    "We're going to arbitrarily set the loss penalty to be: `1,2,3,4` corresponding with each round of betting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequenceLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequence LSTM for choosing to call per round\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 300\n",
    "        self.lstm = nn.LSTMCell(52, self.hidden_size)\n",
    "        self.output = nn.Linear(self.hidden_size, 2)\n",
    "        self.squash = nn.Sigmoid()\n",
    "        self.penalty = [1,2,3,4]\n",
    "    \n",
    "    def forward(self, X): \n",
    "        \"\"\"\n",
    "        X is of shape N, 4, 52\n",
    "        returns a tensor of shape (4) where 0 indicates staying in and 1 indicates folding\n",
    "        \"\"\"\n",
    "        N, _, _ = X.shape\n",
    "        hand = X[:, 0]\n",
    "        flop = X[:, 1]\n",
    "        turn = X[:, 2]\n",
    "        river = X[:, 3]\n",
    "        out = []\n",
    "        \n",
    "        # now each of the objects above is of shape Nx52\n",
    "        hidden, cell = torch.zeros(N, 300, dtype=X.dtype, device=X.device), torch.zeros(N, 300, dtype=X.dtype, device=X.device)\n",
    "        \n",
    "        #pre-flop:\n",
    "        hidden, cell = self.lstm(hand, (hidden, cell))\n",
    "        score = self.output(hidden).argmax(1)\n",
    "        out.append(score)\n",
    "        \n",
    "        #flop:\n",
    "        hidden, cell = self.lstm(flop, (hidden, cell))\n",
    "        score = self.output(hidden).argmax(1)\n",
    "        out.append(score)\n",
    "        \n",
    "        #turn:\n",
    "        hidden, cell = self.lstm(turn, (hidden, cell))\n",
    "        score = self.output(hidden).argmax(1)\n",
    "        out.append(score)\n",
    "        \n",
    "        #river:\n",
    "        hidden, cell = self.lstm(river, (hidden, cell))\n",
    "        score = self.output(hidden).argmax(1)\n",
    "        out.append(score)\n",
    "        \n",
    "        return torch.stack(out, 1) #should be of shape Nx4\n",
    "    \n",
    "    def calc_loss(self, X):\n",
    "        output = self.forward(X)\n",
    "        loss = 0\n",
    "        for i in range(output.shape[0]): #N\n",
    "            for j in range(output.shape[1]): #4\n",
    "                if output[i,j] == 1:\n",
    "                    break\n",
    "                loss += self.penalty[j]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = sequenceLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = utils.make_staged_games(1000, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sequence(X, model, epochs = 10, verbose = False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: training tensor of shape (N,52)\n",
    "        y: target tensor of shape (N,1)\n",
    "        model: a torch.nn.Module model\n",
    "        epochs: number of epochs to train\n",
    "        verbose: iterations of epochs to print out (1 for all, False for none)\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        loss = model.calc_loss(X)\n",
    "        with torch.no_grad():\n",
    "            optimzer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if verbose and e%verbose == 0:\n",
    "                print(\"epoch: {} loss: {:.4f}\".format(e,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-03a76303d82d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-210367f10cb1>\u001b[0m in \u001b[0;36mtrain_sequence\u001b[0;34m(X, model, epochs, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2053b5c094ed>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2053b5c094ed>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# now each of the objects above is of shape Nx52\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#pre-flop:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "train_sequence(X_train, seq, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
