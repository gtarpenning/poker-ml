{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepLearning import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import treys\n",
    "from treys import Evaluator\n",
    "from treys import Card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM poker bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so what are we trying to accomplish here.\n",
    "We want to build a naive LSTM poker bot. We need to do three things:\n",
    "\n",
    "- [ ] Encode a poker game in a meaningful way\n",
    "\n",
    "- [ ] Develop a useful loss metric for games\n",
    "\n",
    "- [ ] Test many-to-many or many-to-one models for games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding a poker game:\n",
    "\n",
    "We're going to approach this by just encoding the cards at each step.\n",
    "There are 4 discrete betting stages that are worth keeping track of in poker:\n",
    "- Pre-Flop\n",
    "- Flop\n",
    "- Turn\n",
    "- River\n",
    "\n",
    "We'll create a `1x4x52` dim tensor to encode all the information in each stage for in one poker game.\n",
    "\n",
    "This way we can create a training dataframe of `N` examples with shape: `Nx4x52`\n",
    "Where `4` again represents the discrete betting stages and `52` is the one hot encoded cards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_staged_games(num_games, device = 'cpu', dtype = torch.float):\n",
    "    to = {'device': device, 'dtype':dtype}\n",
    "    X = []\n",
    "    y = []\n",
    "    for g in range(num_games//2):\n",
    "        p1, p2, board = utils.make_heads_up()\n",
    "        g1 = torch.stack(\n",
    "            [\n",
    "                utils.one_hot_cards(p1, **to), \n",
    "                utils.one_hot_cards(board[:3], **to), \n",
    "                utils.one_hot_cards([board[3]], **to),\n",
    "                utils.one_hot_cards([board[4]], **to)\n",
    "            ]\n",
    "        )\n",
    "        g2 = torch.stack(\n",
    "            [\n",
    "                utils.one_hot_cards(p2, **to), \n",
    "                utils.one_hot_cards(board[:3], **to), \n",
    "                utils.one_hot_cards([board[3]], **to),\n",
    "                utils.one_hot_cards([board[4]], **to)\n",
    "            ]\n",
    "        )\n",
    "        X.append(g1)\n",
    "        X.append(g2)\n",
    "        \n",
    "        s1, s2 = utils.score_heads_up(p1, p2, board)\n",
    "        \n",
    "        y.append(torch.tensor(s2, **to)) # if p1 wins append 0 => s2 is 0\n",
    "        y.append(torch.tensor(s1, **to)) # if p1 loses append 1 => s1 is 1\n",
    "        \n",
    "    X = torch.stack(X)\n",
    "    y = torch.stack(y)\n",
    "    \n",
    "    return X, y.to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to One architecture experiments:\n",
    "\n",
    "We're going to try to use a Many-to-one LSTM to predict our likelihood of winning a hand.\n",
    "\n",
    "Our approach is as follows:\n",
    "- LSTM with a hidden dimension of 300 (arbitrary)\n",
    "- Output transformation with the following properties:\n",
    "    - Linear layer with input 300 and output 2\n",
    "    - Softmax for thresholding \n",
    "    - First dim of output is likelihood for us to win\n",
    "    - Second dim is likelihood for opponents to win\n",
    "    - This allows us to use `CrossEntropyLoss` as a penalty mechanism (Note we'll have to skip the softmax when we use `CrossEntropyLoss`)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A simpleLSTM for poker\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size = 300):\n",
    "        \"\"\"\n",
    "        hidden_size: size of hidden dimension of LSTM\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTMCell(52, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 2)\n",
    "        self.squash = nn.Sigmoid()\n",
    "#         self.params = []\n",
    "#         self.params += list(self.lstm.parameters()) + list(self.output.parameters()) + list(self.squash.parameters())\n",
    "        \n",
    "    def forward(self, X, squash = False):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM\n",
    "        X: input of shape Nx4x52\n",
    "        squash: whether or not to squash using softmax\n",
    "        \n",
    "        Returns:\n",
    "            tensor of shape Nx2 representing likelihood to win and likelihood for opponent to win.\n",
    "            IF squash==True values represent probabilities.\n",
    "        \"\"\"\n",
    "        N, r, _ = X.shape\n",
    "        \n",
    "        hand = X[:,0]\n",
    "        flop = X[:,1]\n",
    "        turn = X[:,2]\n",
    "        river = X[:,3]\n",
    "        \n",
    "        # now we can pass through:\n",
    "        hidden, cell = torch.zeros(N, 300, dtype=X.dtype, device=X.device), torch.zeros(N, 300, dtype=X.dtype, device=X.device)\n",
    "        \n",
    "        # first round:\n",
    "        hidden, cell = self.lstm(hand, (hidden,cell))\n",
    "        \n",
    "        # second round:\n",
    "        hidden, cell = self.lstm(flop, (hidden, cell))\n",
    "        \n",
    "        # third round:\n",
    "        hidden, cell = self.lstm(turn, (hidden, cell))\n",
    "        \n",
    "        # fourth round:\n",
    "        hidden, cell = self.lstm(river, (hidden, cell))\n",
    "        \n",
    "        #output:\n",
    "        scores = self.output(hidden)\n",
    "        \n",
    "        if squash:\n",
    "            return self.squash(scores)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model, epochs = 10, verbose = False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: training tensor of shape (N,52)\n",
    "        y: target tensor of shape (N,1)\n",
    "        model: a torch.nn.Module model\n",
    "        epochs: number of epochs to train\n",
    "        verbose: iterations of epochs to print out (1 for all, False for none)\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        scores = model(X)\n",
    "        loss = criterion(scores, y)\n",
    "        with torch.no_grad():\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if verbose and e%verbose == 0:\n",
    "                print(\"epoch: {} loss: {:.4f}\".format(e,loss.item()))\n",
    "                \n",
    "def evaluate_model(X, y, model):\n",
    "    with torch.no_grad():\n",
    "        scores = model(X)\n",
    "        #threshold for predictions\n",
    "        scores[scores < 0.5] = 0\n",
    "        scores[scores >= 0.5] = 1\n",
    "        return (scores == y).sum().item()/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING THIS TAKES A LONG TIME\n",
    "X_train, y_train = make_staged_games(30000, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = make_staged_games(1000, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleLSTM().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.5587\n",
      "epoch: 100 loss: 0.6666\n",
      "epoch: 200 loss: 0.6477\n",
      "epoch: 300 loss: 0.6878\n",
      "epoch: 400 loss: 0.6623\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train, y_train, model, 500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.629"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out == y_test).sum().item()/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deepLearning/lstmTrain.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deepLearning/lstmTrain_y.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
